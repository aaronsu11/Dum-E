# ü¶æ Dum-E | The Embodied AI Agent

<div align="center">

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Discord](https://dcbadge.limes.pink/api/server/https://discord.gg/g52WMMMQyT?style=flat)](https://discord.gg/g52WMMMQyT)
[![X Follow](https://img.shields.io/twitter/follow/aaronsu1106?style=social)](https://twitter.com/aaronsu1106)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/aaronsu11/Dum-E)

*What if your favorite AI agents have hands?*

[üöÄ Get Started](#-get-started) ‚Ä¢ [üìñ Documentation](https://deepwiki.com/aaronsu11/Dum-E) ‚Ä¢ [üó∫Ô∏è Roadmap](#%EF%B8%8F-roadmap) ‚Ä¢ [ü§ù Contributing](#-contributing)

</div>

---

## üéØ Mission

Inspired by Tony Stark's robotic assistant [Dum-E](https://marvelcinematicuniverse.fandom.com/wiki/Dum-E_and_U), the mission of this project is to create an intelligent, voice & vision enabled AI agent with robotic arm(s) capable of real-time human interaction, physical operations, and orchestration of a wide range of tools and services.

### ‚ú® Key Features

- üé§ **Real-time Voice Interface** - Natural voice conversation with configurable voice and language support
- üß† **Long-horizon Tasks Planning** - Orchestrated by state-of-the-art VLMs with multi-modal reasoning and tool use
- üì° **Asynchronous Tasks Execution** - Support for asynchronous task execution with streaming updates
- üëÅÔ∏è **Hybrid Robot Control** - deep learning policies for generalized physical operations combined with classical control for precise manipulation
- üîß **Modular Architecture** - Flexible interfaces to add your own custom embodiments and tools
- üåê **MCP Protocol Support** - Agents and tools available via MCP for custom client integration (coming soon)

## üì∫ Demo

>As part of the [LeRobot Worldwide Hackathon](https://huggingface.co/LeRobot-worldwide-hackathon) for grabbing fries.

<div align="center">

<video src="https://huggingface.co/datasets/LeRobot-worldwide-hackathon/submissions/resolve/main/191-Dum-E.mp4" controls></video>

</div>

## üöÄ Get Started

### ‚öôÔ∏è Recommended System Requirements

#### Hardware
- **Robot**: SO100/101 robotic arm assembled with a wrist camera
- **Webcam**: External 1080p USB webcam for flexible vision input
- **GPU**: RTX 3060 (12GB VRAM) or better for local policy inference

#### Software
- **OS**: Linux (Ubuntu 22.04+) or Windows with WSL2
- **Python**: 3.10

### üîß Installation

#### üì¶ Prerequisites

1. Install required system dependencies

    ```bash
    sudo apt-get update
    sudo apt-get install ffmpeg libsm6 libxext6
    ```

2. Install CUDA toolkit 12.4

    ```bash
    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
    sudo dpkg -i cuda-keyring_1.1-1_all.deb
    sudo apt-get update
    sudo apt-get -y install cuda-toolkit-12-4
    ```

3. Create a new [conda](https://www.anaconda.com/docs/getting-started/miniconda/install) or venv environment using Python 3.10, for example:

    ```bash
    conda create -n dum-e python=3.10
    conda activate dum-e
    ```

4. Download LeRobot and Isaac-GR00T source code:

    ```bash
    git clone https://github.com/huggingface/lerobot.git
    git clone https://github.com/NVIDIA/Isaac-GR00T
    ```

5. Make sure you are in the conda / venv environment and install LeRobot:

    ```bash
    # conda activate dum-e
    cd lerobot
    # use the tested version on 9th August 2025
    git checkout 0878c6880fa4fbadf0742751cf7b015f2d63a769
    # install with the feetech extension (for SO-ARM10x)
    pip install -e ".[feetech]"
    ```
    Calibrate the robot following the instructions for [SO-100](https://github.com/huggingface/lerobot/blob/b536f47e3ff8c3b340fc5efa52f0ece0a7212a57/examples/10_use_so100.md) or [SO-101](https://github.com/huggingface/lerobot/blob/b536f47e3ff8c3b340fc5efa52f0ece0a7212a57/examples/12_use_so101.md). If you already calibrated the robot before, you can simply copy and paste the `.cache` folder that includes `main_follower.json` to the root of the Dum-E repository later.

6. Install Isaac-GR00T:

    ```bash
    # conda activate dum-e
    cd ../Isaac-GR00T
    # use the tested version on 7th Aug 2025
    git checkout ae7d46f02cdca5d9c80efc446fe41fe2b58e94c7
    pip install --upgrade setuptools
    pip install -e .[base]
    pip install --no-build-isolation flash-attn==2.7.1.post4
    ```
    Download a fine-tuned GR00T model from Hugging Face for the task you want to perform. For example, to pick up a fruit (apple, banana, orange, etc.) and put it on the plate, you can download our checkpoint by running:
    ```bash
    hf download aaronsu11/GR00T-N1.5-3B-FT-FRUIT-0809 --local-dir ./GR00T-N1.5-3B-FT --exclude "optimizer.pt"
    ```

#### ü¶æ Dum-E

1. **Clone the Repository**

    ```bash
    git clone https://github.com/aaronsu11/Dum-E.git
    cd Dum-E
    ```

2. **Install Dum-E Dependencies**
    
    ```bash
    # conda activate dum-e
    pip install -r requirements.txt
    ```

3. **Start policy server**
    
    In a new terminal, run the following command to start a local policy server from the Isaac-GR00T repository:
    ```bash
    cd <path-to-Isaac-GR00T>
    python scripts/inference_service.py \
    --server \
    --model_path ./GR00T-N1.5-3B-FT \
    --embodiment_tag new_embodiment \
    --data_config so100_dualcam
    ```
    This needs to be running in the background as long as you are using the policy for inference.

4. **Test policy execution**
    
    Back in the Dum-E terminal/repository, run the following command by replacing `<robot_serial_port>`, `<wrist_cam_idx>` and `<front_cam_idx>` with your own port and camera indices:
    ```bash
    python -m embodiment.so_arm10x.client \
        --robot_port <robot_serial_port> \
        --robot_type so101_follower \
        --robot_id so101_follower_arm \
        --wrist_cam_idx <wrist_cam_idx> \
        --front_cam_idx <front_cam_idx> \
        --policy_host localhost \
        --lang_instruction "Grab a banana and put it on the plate"
    ```

5. **Environment Configuration**

    Sign up for free accounts at [ElevenLabs](https://elevenlabs.io/), [Deepgram](https://deepgram.com/), [Anthropic](https://www.anthropic.com/api) and obtain the API keys. Then copy the environment template and update the `.env` file with your credentials:
    ```bash
    cp .env.example .env
    ```
    Edit `.env` with your credentials:
    - Anthropic API key for LLM
    - ElevenLabs API key for TTS  
    - Deepgram API key for STT

6. **Test the robot agent**

    Once the policy is working, you can test the robot agent with text instructions by running the following command:
    ```bash
    python -m embodiment.so_arm10x.agent \
        --port <robot_serial_port> \
        --id so101_follower_arm \
        --wrist_cam_idx <wrist_cam_idx> \
        --front_cam_idx <front_cam_idx> \
        --policy_host localhost \
        --instruction "<your-instruction>"
    ```

7. **Start Dum-E**

    Finally, let's start Dum-E!
    ```bash
    python dum_e.py
    ```
    This will launch a web interface at `http://localhost:7860` where you can connect and speak to Dum-E using your microphone. Have fun!

## üèóÔ∏è Architecture Overview

### Core Components

```mermaid
graph TB
    subgraph Interface Layer
        A[Voice Interface<br/>Pipecat]
        B[Third-party Clients<br/>Dum-E MCP Server]
    end

    subgraph Agent Layer
        C[Robot Agent<br/>Reasoning VLM]
        D[Task Manager<br/>Lifecycle Tracking]
        E[Tool Registry<br/>Shared Tools]
        F[Event Publisher<br/>Streaming Updates]
    end

    subgraph Tool Layer
        G[Classical Control<br/>MoveIt]
        H[VLA Policy<br/>Gr00t Inference]
        I[Custom Tools<br/>Public MCP Servers]
    end

    subgraph Hardware Layer
        J[Physical Robot<br/>SO10x]
    end

    A --> C
    B --> C
    C --> D
    C --> E
    C --> F
    E --> G
    E --> H 
    E --> I
    G --> J
    H --> J
```

### üîÑ Data Flow

1. **Voice Interaction** ‚Üí Voice / Vision Streams ‚Üí Pipecat ‚Üí Conversation and Task Delegation
2. **Task Planning** ‚Üí Task Manager ‚Üí Reasoning VLM ‚Üí Multi-step Instruction Breakdown  
3. **Task Execution** ‚Üí Tool Registry ‚Üí Hardware Interface ‚Üí SO10xRobot
4. **Robot Control** ‚Üí Camera Images ‚Üí DL Policy / Classical Control ‚Üí Joint Commands
5. **Feedback Loop** ‚Üí Progress Events ‚Üí Model Context ‚Üí Voice Updates

## üó∫Ô∏è Roadmap

### Q3 2025
- [ ] **Voice Interaction**
  - [ ] Multi-language support (Mandarin, Japanese, Spanish etc.)
  - [ ] Emotional understanding with speech-to-speech models

- [ ] **MCP Servers**
  - [ ] Access to robot agent via MCP
  - [ ] Configurable MCP server endpoints for Dum-E

- [ ] **Local Model Support**
  - [ ] Integration with Ollama for local language model inference

- [x] **Upgrade Dependency**
  - [x] LeRobot new hardware APIs

### Q4 2025

- [ ] **Cross-Platform Support**
  - [ ] Docker containers for platform-agnostic deployment

- [ ] **ROS2 Integration**
  - [ ] Native ROS2 node implementation
  - [ ] Integration with existing ROS toolkits

### üî¨ Ongoing: Research Initiatives
- [ ] **Embodied AI Research**
  - [ ] Generalizable & scalable policies for physical tasks
  - [ ] Efficient dataset collection and training

- [ ] **Human-Robot Interaction**
  - [ ] Natural multi-modal understanding
  - [ ] Contextual conversation memory
  - [ ] Self-evolving personality and skillset

## ü§ù Contributing

We welcome contributions from the robotics and AI community! Here's how you can help:

### üåü Ways to Contribute

- **üêõ Bug Reports**: Found an issue? Create a detailed bug report
- **üí° Feature Requests**: Have ideas? Share them in our discussions or Discord
- **üìù Documentation**: Help improve our docs and tutorials
- **üß™ Testing**: Add test cases and improve coverage
- **üöÄ Code**: Submit pull requests with new features or fixes

### üìã Development Guidelines

1. **Fork the Repository**
   ```bash
   git fork https://github.com/aaronsu11/Dum-E.git
   cd Dum-E
   git checkout -b feature/issue-<number>/<your-feature-name>
   ```

2. **Follow Code Standards**
   - Use Python 3.10 type hints
   - Follow PEP 8 style guidelines
   - Add comprehensive docstrings
   - Maintain test coverage >95%

3. **Testing Requirements**
   ```bash
   # Run tests before submitting
   python -m pytest tests/
   python -m black <your-file-or-directory>
   python -m isort <your-file-or-directory>
   ```

4. **Pull Request Process**
   - Create detailed PR description
   - Link related issues
   - Ensure CI/CD passes
   - Request review from maintainers

### üë• Community

- **Discord**: [Join our community](https://discord.gg/g52WMMMQyT)
- **GitHub Discussions**: Share ideas and get help

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

This project builds on top of the following open-source projects:

- [Pipecat Framework](https://github.com/pipecat-ai/pipecat/blob/main/LICENSE)
- [Strands Agents](https://github.com/strands-agents/sdk-python/blob/main/LICENSE)
- [Isaac GR00T](https://github.com/NVIDIA/Isaac-GR00T/blob/main/LICENSE)
- [LeRobot](https://github.com/huggingface/lerobot/blob/main/LICENSE)

---

<div align="center">

**‚≠ê Star us on GitHub ‚Äî it motivates us a lot!**

[üöÄ Get Started](#-get-started) ‚Ä¢ [üìñ Documentation](https://deepwiki.com/aaronsu11/Dum-E) ‚Ä¢ [ü§ù Join Community](https://discord.gg/g52WMMMQyT) ‚Ä¢ [üíº Commercial Use](mailto:aaronsu11@outlook.com)

*Built with ‚ù§Ô∏è for the future of robotics*

</div> 